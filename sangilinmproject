import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import pickle
import spacy
from transformers import pipeline

# Initialize NLP tools
nltk.download('stopwords')
nltk.download('wordnet')
nlp = spacy.load('en_core_web_sm')
sentiment_analyzer = pipeline("sentiment-analysis")

class FakeNewsDetector:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)
        self.model = PassiveAggressiveClassifier(max_iter=100)
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
        
    def preprocess_text(self, text):
        # Lowercase
        text = text.lower()
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        # Remove user @ references and '#' from tweet
        text = re.sub(r'\@\w+|\#', '', text)
        # Remove punctuation
        text = re.sub(r'[^\w\s]', '', text)
        # Tokenize
        words = text.split()
        # Lemmatize and remove stopwords
        words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]
        return ' '.join(words)
    
    def extract_features(self, text):
        # Sentiment analysis
        sentiment = sentiment_analyzer(text[:512])[0]
        # Named entities
        doc = nlp(text)
        entities = [ent.label_ for ent in doc.ents]
        # Readability scores, word count, etc. could be added here
        return {
            'sentiment_label': sentiment['label'],
            'sentiment_score': sentiment['score'],
            'entity_count': len(entities)
        }
    
    def train(self, X, y):
        # Preprocess text
        X_processed = [self.preprocess_text(text) for text in X]
        # Vectorize text
        X_vectorized = self.vectorizer.fit_transform(X_processed)
        # Train model
        self.model.fit(X_vectorized, y)
        
    def predict(self, text):
        # Preprocess
        processed_text = self.preprocess_text(text)
        # Vectorize
        text_vector = self.vectorizer.transform([processed_text])
        # Predict
        prediction = self.model.predict(text_vector)
        # Get prediction probability
        proba = self.model.decision_function(text_vector)
        # Get additional features
        features = self.extract_features(text)
        
        return {
            'prediction': 'Fake' if prediction[0] == 1 else 'Real',
            'confidence': float(np.abs(proba[0])),
            'sentiment': features['sentiment_label'],
            'sentiment_score': features['sentiment_score'],
            'entities_count': features['entity_count']
        }
    
    def save_model(self, path):
        with open(path, 'wb') as f:
            pickle.dump({
                'vectorizer': self.vectorizer,
                'model': self.model
            }, f)
    
    def load_model(self, path):
        with open(path, 'rb') as f:
            data = pickle.load(f)
            self.vectorizer = data['vectorizer']
            self.model = data['model']

def main():
    # Sample usage
    detector = FakeNewsDetector()
    
    # Load dataset (example - you'll need real data)
    data = pd.read_csv('news_dataset.csv')
    X = data['text']
    y = data['label']
    
    # Train the model
    print("Training model...")
    detector.train(X, y)
    
    # Save model
    detector.save_model('fake_news_model.pkl')
    
    # Test prediction
    test_news = "Scientists confirm that eating chocolate every day improves longevity by 50 years."
    result = detector.predict(test_news)
    
    print("\nPrediction Results:")
    print(f"Text: {test_news}")
    print(f"Prediction: {result['prediction']}")
    print(f"Confidence: {result['confidence']:.2f}")
    print(f"Sentiment: {result['sentiment']} ({result['sentiment_score']:.2f})")
    print(f"Named Entities Count: {result['entities_count']}")

if __name__ == "__main__":
    main()
